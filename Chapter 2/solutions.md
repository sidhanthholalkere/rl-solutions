# Chapter 2

1. In epsilon-greedy action selection, for the case of two actions and eps = 0.5, what is
the probability that the greedy action is selected?
    - There is a 50% chance that we select the greedy action and then out of the remaining 50% for which we randomly sampled, another 50% that we select the greedy action. This leaves the probability of 0.5 + 0.5 * 0.5 = 0.75.

2. *Bandit example* Consider a k-armed bandit problem with k = 4 actions, denoted 1, 2, 3, and 4. Consider applying to this problem a bandit algorithm using epsilon-greedy action selection, sample-average action-value estimates, and initial estimates of Q_1(a) = 0, for all a. Suppose the initial sequence of actions and rewards is A_1 = 1, R_1 = -1, A_2 = 2, R_2 = 1, A_3 = 2, R_3 = -2, A_4 = 2, R_4 = 2, A_5 = 3, R_5 = 0. On some of these timesteps the epsilon case may have occurred, causing an action to be selected at random. On which time steps did this definitely occur? On which time steps could this have possibly occurred?
    - So A_1 could be either since we don't know whether 1 was the greedy option or not. A_2 could be either as well since we still don't know what the greedy option is. A_3 must have been an epsilon step since otherwise it would've chosen 2 since its given the highest reward so far. A_4 could be either since even though it is the greedy option, it could've been randomly selected if it was an epsilon choice. A_5 must also have been an epsilon step since the greedy option is 2.

3. In the comparison shown in Figure 2.2, which method will perform best in the long run in terms of cumulative reward and probability of selecting the best action? How much better will it be? Express your answer quantitatively.
    - For a while, eps=0.1 will be ahead but in the extremely long run eps=0.01 will perform best in cumulative reward and probability of selecting the best action since once the agent knows the real state values to a certain accuracy, the eps=0.01 model is less likely to chose a random option and more likely to choose the optimal option. It will choose the best action around 9% of the time more than the eps=0.1 model.

4. If the step-size parameters, alpha_n, are not constant, then the estimate Q_n is a weighted average of previously recieved rewards with a weighting different from that given by (2.6). What is the weighting on each prior reward for the general case, analogous to (2.6) in terms of the sequence of step-size parameters.
    - So Q_n = Q_n-1 + alpha_n[R_n-1 - Q_n-1]. Which when expanded, each R_i is weighted by (1-a_n)(1-a_n-1)...(1-a_i+1).

5. *(programming)* Design and conduct an experiment to demonstrate the difficulties that sample-average methods have for nonstationary problems. Use a modified version of the 10-armed testbed in which all the q_*(a) start out equal and then take independent random walks (say by adding a normally distributed increment with mean zero and standard devation 0.01 to all the q_*(a) on each step). Prepare plots like Figure 2.2 for an action-value method using sample averages, incrementally computed, and another action-value method of using a constant step-size parameter, alpha = 0.1. Use epsilon = 0.1 and longer runs, say of 10,000 steps.
    - See 2-2.py for figure generation. Each figure was generated by averaging 100 runs for each method. 2-2-average-rewards.png displays the average reward over time and 2-2-optimal-percentage.png displays how often each model chose the optimal move

6. *Mysterious Spikes* The results shown in Figure 2.3 should be quite reliable because they are averages over 2000 individual, randomly chosen 10-armed bandit tasks. Why, then, are there oscillations and spikes in the early part of the curve for the optimistic method? In other words, what might make this method perform particularly better or worse on average, on particular early steps?
    - There are oscillations and spikes in the early part of the curve for optimistic methods because it takes longer for Q to approach q since its initially offset in the optimistic direction, leaving it to "latch" on to various actions for longer than usual. This method will perform particularly worse on average for earlier steps because it takes longer for Q to approach q so the agent, for earlier steps, will have a less accurate Q than if it was initially neutral.

2.7 *Unbiased Constant-Step-Size Trick* In most of this chapter we have used sample averages to estimate action values because sample averages do not produce the initial bias that constant step sizes do (see the analysis leading to (2.6)). However, sample averages are not a completely satisfactory solution because they may perform poorly on nonstationary problems. Is it possible to avoid the bias of constant step sizes while retaining their advantages on nonstationary problems? One way is to use a stepsize of Beta_n = alpha / o_avg_n, to process the nth reward for a particular action, where alpha > 0 is a conventional constant step size and o_avg_n is a trace of one that starts at 0: o_avg_n = o_avg_n-1 + alpha(1 - o_avg_n-1) for n >= 0, with o_avg_0 = 0. Carry out analysis like that in 2.6 to show that Q_n is an exponentially recency-weighted average without initial bias.